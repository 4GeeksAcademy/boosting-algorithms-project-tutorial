<!-- hide -->
# Boosting algorithm Project Tutorial
<!-- endhide -->

- So far, we have modeled Titanic data with Logistic Regression and with Random Forest. In this project we will continue the modeling part of Titanic by creating a last model a gradient boosting algorithm: XGBoost!


## üå±  How to start this project

You will not create a new repository this time. Continue your Titanic project by creating a new modeling part for XGBoost (after your random forest results).

## üöõ How to deliver this project

Once you are finished creating your model, make sure to commit your changes, push to your repository and go to 4Geeks.com to upload the repository link again (same link you delivered in the previous project).

## üìù Instructions

**Predicting Titanic survival using XGBoost**

We need to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc). To be able to predict which passengers were more likely to survive we will use XGBoost to train the model.

**Step 1:**

Build a new predictive model (don't erase the previous one) using XGBoost. 

**Step 2:**

Using the same evaluation metric as last project, evaluate your new XGBoost model.
Optimize your model hyperparameters. The full list of possible parameters can be found in the following link: https://xgboost.readthedocs.io/en/latest/parameter.html

**Step 3:**

Use the app.py to create your new pipeline. 

Save your final XGBoost model in the 'models' folder.

In your README file write a brief summary.
